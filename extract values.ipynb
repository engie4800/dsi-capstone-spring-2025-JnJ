{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, all pdfs have the section Interpretation and conclusions of economic evidence, so we aim to find that section. Since reference always follows this section, we stop at reference.\n",
    "\n",
    "TODO:\n",
    "For pdfs without section Interpretation and conclusions of economic evidence, we need NLP method to determine start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting section: B.3.15 Interpretation and conclusions of economic evidence\n",
      "B.3.15 Interpretation and conclusions of economic evidence\n",
      "Summary of cost-effectiveness evidence\n",
      "The cost-effectiveness of baricitinib in severe AA was evaluated versus ‘watch and wait’, the\n",
      "most clinically relevant comparator for this population, due to a lack of high-quality clinical data\n",
      "for another comparator. In the probabilistic base case, baricitinib was associated with higher\n",
      "costs (£***** per patient over a lifetime horizon) and higher benefits (**** QALYs per patient over\n",
      "a lifetime horizon) compared with ‘watch and wait’. The base case probabilistic ICER was\n",
      "£29,111 per QALY gained and did not differ meaningfully from the deterministic ICER (£29,395\n",
      "per QALY gained). In absolute terms, base case probabilistic results suggested that baricitinib\n",
      "4mg was associated with a total cost of £******, of which £****** related to drug acquisition. These\n",
      "were partially compensated by cost savings due to reduced disease management costs,\n",
      "psychological burden costs and BSC drug monitoring costs. The PSA results indicated that\n",
      "baricitinib was ****% likely to be cost-effective at a cost-effectiveness threshold of £30,000 per\n",
      "QALY gained.\n",
      "Overall, the results indicate that baricitinib to be a cost-effective option for the treatment of\n",
      "severe AA within the NHS versus ‘watch and wait’.\n",
      "Strengths\n",
      "The cost-effectiveness model developed for this submission has several strengths. The efficacy\n",
      "of baricitinib is based largely on robust phase III clinical trial data derived from two clinical trials.\n",
      "The model was built to align with the NICE reference case, adopting an NHS and PSS\n",
      "perspective, a lifetime time horizon to fully capture all costs and QALY gains associated with the\n",
      "interventions, and discount rates for costs and benefits of 3.5%. Utility data were elicited directly\n",
      "from AA patients in Europe, including the UK and were cross-walked from the EQ-5D-5L to the\n",
      "EQ-5D-3L, in line with the latest NICE guidance.\n",
      "Limitations\n",
      "There are some limitations to the current modelling approach that should be considered. Firstly,\n",
      "there was no previously published economic model for AA and therefore the current model\n",
      "structure was based on similar models in other dermatological disorders. The model structure\n",
      "and the key assumptions have been tested with clinical experts to ensure the model adequately\n",
      "captures disease progression and the main clinical and economics aspects of the disease,\n",
      "however, there remains an extent of uncertainty as to how well the model predicts the cost-\n",
      "effectiveness of baricitinib for the treatment of AA. Second, discontinuation rates for baricitinib\n",
      "and ‘watch and wait’ were based on data from the phase III studies with follow-up until Week 52\n",
      "and Week 36 respectively. Given that these rates in the maintenance phase were used from\n",
      "Week 36 and beyond to estimate the probability of remaining in maintenance, there is inherent\n",
      "uncertainty on long-term discontinuation rates and the predictions they make. Third, the BSC\n",
      "health state assumes that patients would not benefit from treatment despite receiving a basket of\n",
      "various off-label therapies for the management of AA. This approach can underestimate the\n",
      "benefit that these patients exhibit from the BSC treatments, however, given the lack of\n",
      "comparative efficacy data for these treatments it was not possible to estimate their impact in\n",
      "efficacy and HRQoL terms. In addition to this, given the frequency of AEs and high relapse rates\n",
      "associated with such therapies, the extent of benefit of BSC for patients with AA is questionable.\n",
      "Company evidence submission template for baricitinib for treating severe alopecia areata\n",
      "[ID3979]\n",
      "© Eli Lilly and Company Limited (2022). All rights reserved Page 144 of 151\n",
      "Conclusion\n",
      "Notwithstanding the limitations mentioned above, based on the currently available evidence, the\n",
      "results of the analyses conducted show, with a good degree of certainty, that baricitinib 4 mg is a\n",
      "clinically- and cost-effective alternative to ‘watch and wait’ in patients with severe AA.\n",
      "Company evidence submission template for baricitinib for treating severe alopecia areata\n",
      "[ID3979]\n",
      "© Eli Lilly and Company Limited (2022). All rights reserved Page 145 of 151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def extract_section_from_pdf(pdf_path, target_section=\"Interpretation and conclusions of economic evidence\", start_page=20):\n",
    "    \"\"\"\n",
    "    Extracts the section with a name similar to 'target_section' from the given PDF, starting from 'start_page'.\n",
    "    Works even if the section name is slightly different.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in range(start_page - 1, len(pdf.pages)):  # Adjust for 0-based index\n",
    "            text.append(pdf.pages[page_num].extract_text())\n",
    "\n",
    "    full_text = \"\\n\".join(filter(None, text))  # Remove None values\n",
    "\n",
    "    # Extract all section headers using a regex pattern for headings (e.g., \"B.x.x Section Name\")\n",
    "    section_pattern = r\"(B\\.\\d+\\.\\d+.*?)\\n\"\n",
    "    section_matches = re.findall(section_pattern, full_text)\n",
    "\n",
    "    if not section_matches:\n",
    "        print(\"No section headers found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the best match for the target section\n",
    "    best_match, score = process.extractOne(target_section, section_matches)\n",
    "\n",
    "    if score < 80:  # Adjust threshold based on quality\n",
    "        print(f\"No close match found for '{target_section}'. Closest match: '{best_match}' (Score: {score})\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Extracting section: {best_match}\")\n",
    "\n",
    "    # Find start position of matched section\n",
    "    start_pos = full_text.find(best_match)\n",
    "\n",
    "    # Find Reference to determine the end position\n",
    "    next_match = re.search(r\"B\\.\\d+\\s+References\", full_text[start_pos + len(best_match):])\n",
    "\n",
    "    if next_match:\n",
    "        end_pos = start_pos + len(best_match) + next_match.start()\n",
    "        extracted_text = full_text[start_pos:end_pos]\n",
    "    else:\n",
    "        extracted_text = full_text[start_pos:]  # Extract till the end if no next header found\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"10 Typical Committee Papers/committee-papers-Baricitinib-not recommend.pdf\"\n",
    "extracted_text = extract_section_from_pdf(pdf_path, start_page=20) # Starts at page 20 because all titles are presented in the content section\n",
    "\n",
    "if extracted_text:\n",
    "    print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract pdf section\n",
    "2. Run pdf section through NLP\n",
    "3. Extract website section from website (committee)\n",
    "4. Run website section using NLP\n",
    "5. Use summarized website section to test accuracy of pdf section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only run the next part with GPU!\n",
    "A few models we could look at:\n",
    "1. MedAlpaca-7b\n",
    "2. OpenBioLLM-8B\n",
    "3. LLaMA2 base model (Matt has gotten meta's verification)\n",
    "\n",
    "Currently tried to run these with Google Colab's T4 but struggled, even though we are only choosing these smaller <10B models (OpenBioLLM has a 70B model that works better than ChatGPT). With our current GPU resources and VRAM, running these are especially difficults. Next step is to consider using ChatGPT instead to overcome the limitation to computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/medalpaca-7b\"\n",
    "\n",
    "# Use the next part to download the model\n",
    "#!huggingface-cli download medalpaca/medalpaca-7b --cache-dir $model_path\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = \"/content/drive/MyDrive/medalpaca-7b/models--medalpaca--medalpaca-7b/snapshots/fbb41b75d5a46ba405d496db1083a6f1d3df72a2\" # change this accordingly\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=\"auto\", load_in_8bit=True)\n",
    "\n",
    "print(\" Model loaded successfully from\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Process input text correctly (truncate to 512 tokens)\n",
    "tokenized_input = tokenizer(extracted_text, truncation=True, max_length=500)\n",
    "truncated_text = tokenizer.decode(tokenized_input[\"input_ids\"])\n",
    "\n",
    "# Improved prompt that forces a number response\n",
    "final_prompt = f\"\"\"\n",
    "Question: Based on the economic evidence provided, rate the cost-effectiveness of this treatment on a scale from 1 to 5. \n",
    "1 = Not cost-effective, 5 = Highly cost-effective.\n",
    "Answer ONLY with a single number (1, 2, 3, 4, or 5). Do NOT provide explanations or any other text.\n",
    "\n",
    "Example 1:\n",
    "Economic Evidence: The ICER is above the standard threshold, making the treatment unlikely to be cost-effective.\n",
    "Answer: 2\n",
    "\n",
    "Example 2:\n",
    "Economic Evidence: The treatment provides high QALY gains and remains cost-effective under multiple scenarios.\n",
    "Answer: 5\n",
    "\n",
    "Now, based on the economic evidence below, provide your answer.\n",
    "\n",
    "Economic Evidence:\n",
    "{truncated_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response with max_new_tokens\n",
    "raw_response = text_generator(final_prompt, max_new_tokens=10, do_sample=True, temperature=0.2, top_p=0.9)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# pl = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) \n",
    "\n",
    "# def chunk_text(text, max_length=512, overlap=50):\n",
    "#     tokens = tokenizer.encode(text)\n",
    "#     chunks = []\n",
    "#     for i in range(0, len(tokens), max_length - overlap):\n",
    "#         chunk = tokens[i:i+max_length]\n",
    "#         # Ensure last sentence is included in the next chunk for continuity\n",
    "#         if i > 0:\n",
    "#             chunk = tokenizer.encode(chunks[-1].split(\".\")[-1]) + chunk\n",
    "#         chunks.append(tokenizer.decode(chunk))\n",
    "#     return chunks\n",
    "\n",
    "# # Split text into 512-token chunks with 50-token overlap\n",
    "# chunks = chunk_text(text)\n",
    "\n",
    "# # Summarize each chunk individually\n",
    "# summaries = []\n",
    "# for chunk in chunks:\n",
    "#     prompt = f\"Summarize the cost-effectiveness findings in this section:\\n\\n{chunk}\"\n",
    "#     summary = text_generator(prompt, max_length=64, do_sample=True)[0][\"generated_text\"]\n",
    "#     summaries.append(summary)\n",
    "\n",
    "# # Combine all summaries into a single text\n",
    "# combined_summary = \" \".join(summaries)\n",
    "\n",
    "# # Ask the model to make a final decision\n",
    "# final_prompt = f\"Based on the following economic evaluation summary, determine if the treatment is cost-effective:\\n\\n{combined_summary}\\n\\nAnswer with a value between 1 to 5, with 5 being the most cost-effective.\"\n",
    "\n",
    "# final_response = text_generator(final_prompt, max_length=100, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "# print(\"Final Cost-Effectiveness Verdict:\")\n",
    "# print(final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Microsoft Phi-2 model\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=4096)\n",
    "\n",
    "# Construct a short and effective prompt\n",
    "final_prompt = f\"\"\"\n",
    "\n",
    "Question: Based on the economic evidence provided, rate the cost-effectiveness of this treatment on a scale from 1 to 5. \n",
    "1 = Not cost-effective, 5 = Highly cost-effective.\n",
    "- Answer with a number between 1 and 5.\n",
    "- Do NOT repeat any text.\n",
    "- Provide only a number.\n",
    "\n",
    "Example 1:\n",
    "Economic Evidence: The ICER is above the standard threshold, making the treatment unlikely to be cost-effective.\n",
    "Answer: 2\n",
    "\n",
    "Example 2:\n",
    "Economic Evidence: The treatment provides high QALY gains and remains cost-effective under multiple scenarios.\n",
    "Answer: 5\n",
    "\n",
    "Now, based on the economic evidence below, provide your answer.\n",
    "\n",
    "Economic Evidence:\n",
    "{extracted_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response (limit to 3 tokens)\n",
    "raw_response = text_generator(final_prompt, max_new_tokens=10, do_sample=True, temperature=0.2, top_p=0.9)[0][\"generated_text\"]\n",
    "\n",
    "print(\"Retrieved Relevant Text:\", raw_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ENTER CHATGPT API KEY HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_effectiveness_score(retrieved_text):\n",
    "    \"\"\"\n",
    "    Sends the retrieved economic evaluation text to GPT-4 and asks it to rate cost-effectiveness on a scale of 1-5.\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in health economics and NICE guidelines.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "            Based on the following economic evaluation, rate the cost-effectiveness of the treatment from 1 to 5:\n",
    "            1 = Not cost-effective\n",
    "            5 = Highly cost-effective\n",
    "\n",
    "            {extracted_text}\n",
    "\n",
    "            Answer with ONLY a number (1, 2, 3, 4, or 5). Provide 1 sentence explanations.\n",
    "            \"\"\"}\n",
    "        ],\n",
    "        max_tokens=20,\n",
    "        temperature=0.1  # Low temperature for deterministic output\n",
    "    )\n",
    "\n",
    "    # Extract response\n",
    "    gpt_output = response.choices[0].message.content.strip()\n",
    "\n",
    "    return gpt_output\n",
    "\n",
    "# Example call\n",
    "cost_effectiveness_score = get_cost_effectiveness_score(extracted_text)\n",
    "\n",
    "# Print only the final numerical value\n",
    "print(cost_effectiveness_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
